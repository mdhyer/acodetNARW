
@article{cholewiak_humpback_2013,
	title = {Humpback whale song hierarchical structure: {Historical} context and discussion of current classification issues},
	volume = {29},
	issn = {0001-4966},
	doi = {10.1121/1.4987495},
	number = {3},
	journal = {Marine Mammal Science},
	author = {Cholewiak, Danielle M and Sousa-Lima, Renata S and Cerchio, Salvatore},
	year = {2013},
	note = {Publisher: Wiley Online Library},
	pages = {E312--E332},
}

@article{garland_cultural_2020,
	title = {Cultural transmission, evolution, and revolution in vocal displays: insights from bird and whale song},
	journal = {Frontiers in psychology},
	author = {Garland, Ellen C and McGregor, Peter K},
	year = {2020},
	note = {Publisher: Frontiers},
	pages = {2387},
}

@article{reidenberg_terrestrial_2017,
	title = {Terrestrial, semiaquatic, and fully aquatic mammal sound production mechanisms},
	volume = {13},
	number = {2},
	journal = {Acoust Today},
	author = {Reidenberg, Joy S},
	year = {2017},
	pages = {35--43},
}

@article{garland_devil_2017,
	title = {The devil is in the detail: {Quantifying} vocal variation in a complex, multi-levelled, and rapidly evolving display},
	volume = {142},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Garland, Ellen C and Rendell, Luke and Lilley, Matthew S and Poole, M Michael and Allen, Jenny and Noad, Michael J},
	year = {2017},
	note = {Publisher: Acoustical Society of America},
	pages = {460--472},
}

@article{bermant_deep_2019,
	title = {Deep machine learning techniques for the detection and classification of sperm whale bioacoustics},
	volume = {9},
	number = {1},
	journal = {Scientific reports},
	author = {Bermant, Peter C and Bronstein, Michael M and Wood, Robert J and Gero, Shane and Gruber, David F},
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	pages = {1--10},
}

@book{geron_hands-machine_2019,
	title = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, tools, and techniques to build intelligent systems},
	publisher = {O'Reilly Media, Inc.},
	author = {Géron, Aurélien},
	year = {2019},
	file = {Attachment:/home/vincent/Zotero/storage/G45VDSSX/2019 Hands-On Machine Learning with Scikit Learn, Keras and Tensorflow:application/pdf},
}

@article{stowell_detection_2015,
	title = {Detection and {Classification} of {Acoustic} {Scenes} and {Events}},
	volume = {17},
	issn = {1941-0077},
	doi = {10.1109/TMM.2015.2428998},
	abstract = {For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.},
	number = {10},
	journal = {IEEE Transactions on Multimedia},
	author = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
	month = oct,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Audio databases, event detection, Event detection, Licenses, machine intelligence, Microphones, Music, pattern recognition, Speech, Speech recognition},
	pages = {1733--1746},
	file = {IEEE Xplore Abstract Record:/home/vincent/Zotero/storage/3DT8UZAJ/7100934.html:text/html;IEEE Xplore Full Text PDF:/home/vincent/Zotero/storage/FX8PMBF2/Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf:application/pdf},
}

@article{allen_convolutional_2021,
	title = {A {Convolutional} {Neural} {Network} for {Automated} {Detection} of {Humpback} {Whale} {Song} in a {Diverse}, {Long}-{Term} {Passive} {Acoustic} {Dataset}},
	volume = {8},
	issn = {2296-7745},
	url = {https://www.frontiersin.org/article/10.3389/fmars.2021.607321},
	abstract = {Passive acoustic monitoring is a well-established tool for researching the occurrence, movements, and ecology of a wide variety of marine mammal species. Advances in hardware and data collection have exponentially increased the volumes of passive acoustic data collected, such that discoveries are now limited by the time required to analyze rather than collect the data. In order to address this limitation, we trained a deep convolutional neural network (CNN) to identify humpback whale song in over 187,000 h of acoustic data collected at 13 different monitoring sites in the North Pacific over a 14-year period. The model successfully detected 75 s audio segments containing humpback song with an average precision of 0.97 and average area under the receiver operating characteristic curve (AUC-ROC) of 0.992. The model output was used to analyze spatial and temporal patterns of humpback song, corroborating known seasonal patterns in the Hawaiian and Mariana Islands, including occurrence at remote monitoring sites beyond well-studied aggregations, as well as novel discovery of humpback whale song at Kingman Reef, at 5∘ North latitude. This study demonstrates the ability of a CNN trained on a small dataset to generalize well to a highly variable signal type across a diverse range of recording and noise conditions. We demonstrate the utility of active learning approaches for creating high-quality models in specialized domains where annotations are rare. These results validate the feasibility of applying deep learning models to identify highly variable signals across broad spatial and temporal scales, enabling new discoveries through combining large datasets with cutting edge tools.},
	urldate = {2022-06-06},
	journal = {Frontiers in Marine Science},
	author = {Allen, Ann N. and Harvey, Matt and Harrell, Lauren and Jansen, Aren and Merkens, Karlina P. and Wall, Carrie C. and Cattiau, Julie and Oleson, Erin M.},
	year = {2021},
	file = {Full Text PDF:/home/vincent/Zotero/storage/RDNE24LK/Allen et al. - 2021 - A Convolutional Neural Network for Automated Detec.pdf:application/pdf},
}

@misc{jansen_unsupervised_2017,
	title = {Unsupervised {Learning} of {Semantic} {Audio} {Representations}},
	url = {http://arxiv.org/abs/1711.02209},
	abstract = {Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several classagnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classiﬁcation loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41\% and 84\% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classiﬁcation tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classiﬁcation performance.},
	language = {en},
	urldate = {2022-06-06},
	publisher = {arXiv},
	author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Jiayang and Moore, R. Channing and Saurous, Rif A.},
	month = nov,
	year = {2017},
	note = {Number: arXiv:1711.02209
arXiv:1711.02209 [cs, eess, stat]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Submitted to ICASSP 2018},
	file = {Jansen et al. - 2017 - Unsupervised Learning of Semantic Audio Representa.pdf:/home/vincent/Zotero/storage/CE4PYHD7/Jansen et al. - 2017 - Unsupervised Learning of Semantic Audio Representa.pdf:application/pdf},
}

@article{mercado_all_2022,
	title = {All units are equal in humpback whale songs, but some are more equal than others},
	volume = {25},
	issn = {1435-9456},
	url = {https://doi.org/10.1007/s10071-021-01539-8},
	doi = {10.1007/s10071-021-01539-8},
	abstract = {Flexible production and perception of vocalizations is linked to an impressive array of cognitive capacities including language acquisition by humans, song learning by birds, biosonar in bats, and vocal imitation by cetaceans. Here, we characterize a portion of the repertoire of one of the most impressive vocalizers in nature: the humpback whale. Qualitative and quantitative analyses of sounds (units) produced by humpback whales revealed that singers gradually morphed streams of units along multiple acoustic dimensions within songs, maintaining the continuity of spectral content across subjectively dissimilar unit “types.” Singers consistently produced some unit forms more frequently and intensely than others, suggesting that units are functionally heterogeneous. The precision with which singing humpback whales continuously adjusted the acoustic characteristics of units shows that they possess exquisite vocal control mechanisms and vocal flexibility beyond what is seen in most animals other than humans. The gradual morphing of units within songs that we observed is inconsistent with past claims that humpback whales construct songs from a fixed repertoire of discrete unit types. These findings challenge the results of past studies based on fixed-unit classification methods and argue for the development of new metrics for characterizing the graded structure of units. The specific vocal variations that singers produced suggest that humpback whale songs are unlikely to provide detailed information about a singer’s reproductive fitness, but can reveal the precise locations and movements of singers from long distances and may enhance the effectiveness of units as sonar signals.},
	language = {en},
	number = {1},
	urldate = {2022-06-10},
	journal = {Animal Cognition},
	author = {Mercado, Eduardo and Perazio, Christina E.},
	month = feb,
	year = {2022},
	keywords = {Acoustic communication, Bioacoustics, Cetacean, Mysticete, Vocal learning},
	pages = {149--177},
	file = {Full Text PDF:/home/vincent/Zotero/storage/SBEEL2TK/Mercado and Perazio - 2022 - All units are equal in humpback whale songs, but s.pdf:application/pdf},
}
